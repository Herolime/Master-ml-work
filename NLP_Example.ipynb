{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b45be95",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60efdbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af165fe1",
   "metadata": {},
   "source": [
    "# I. Load Data\n",
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734e9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "path_files = './Data_Sources'\n",
    "df_raw = pd.read_csv(path_files+'/all-data.csv', encoding= 'latin-1', header=None)\n",
    "df_raw.columns = ['category', 'text']\n",
    "#Shuffle input\n",
    "df_raw = df_raw.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92fc96",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24250a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "vocabulary = [x for x in word_vectors.key_to_index]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322baf0",
   "metadata": {},
   "source": [
    "## Check embeddings of one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c448bb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6298e-01  3.0141e-01  5.7978e-01  6.6548e-02  4.5835e-01 -1.5329e-01\n",
      "  4.3258e-01 -8.9215e-01  5.7747e-01  3.6375e-01  5.6524e-01 -5.6281e-01\n",
      "  3.5659e-01 -3.6096e-01 -9.9662e-02  5.2753e-01  3.8839e-01  9.6185e-01\n",
      "  1.8841e-01  3.0741e-01 -8.7842e-01 -3.2442e-01  1.1202e+00  7.5126e-02\n",
      "  4.2661e-01 -6.0651e-01 -1.3893e-01  4.7862e-02 -4.5158e-01  9.3723e-02\n",
      "  1.7463e-01  1.0962e+00 -1.0044e+00  6.3889e-02  3.8002e-01  2.1109e-01\n",
      " -6.6247e-01 -4.0736e-01  8.9442e-01 -6.0974e-01 -1.8577e-01 -1.9913e-01\n",
      " -6.9226e-01 -3.1806e-01 -7.8565e-01  2.3831e-01  1.2992e-01  8.7721e-02\n",
      "  4.3205e-01 -2.2662e-01  3.1549e-01 -3.1748e-01 -2.4632e-03  1.6615e-01\n",
      "  4.2358e-01 -1.8087e+00 -3.6699e-01  2.3949e-01  2.5458e+00  3.6111e-01\n",
      "  3.9486e-02  4.8607e-01 -3.6974e-01  5.7282e-02 -4.9317e-01  2.2765e-01\n",
      "  7.9966e-01  2.1428e-01  6.9811e-01  1.1262e+00 -1.3526e-01  7.1972e-01\n",
      " -9.9605e-04 -2.6842e-01 -8.3038e-01  2.1780e-01  3.4355e-01  3.7731e-01\n",
      " -4.0251e-01  3.3124e-01  1.2576e+00 -2.7196e-01 -8.6093e-01  9.0053e-02\n",
      " -2.4876e+00  4.5200e-01  6.6945e-01 -5.4648e-01 -1.0324e-01 -1.6979e-01\n",
      "  5.9437e-01  1.1280e+00  7.5755e-01 -5.9160e-02  1.5152e-01 -2.8388e-01\n",
      "  4.9452e-01 -9.1703e-01  9.1289e-01 -3.0927e-01]\n"
     ]
    }
   ],
   "source": [
    "vector = word_vectors['computer']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af182315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(df_input, lemmatizer, word_vectors, vocabulary, col_sentences):\n",
    "    \"\"\"\n",
    "    Función para preprocesar las palabras de entrada y obtener una lista con las matrices de embeddings\n",
    "    de las palabras de cada registro.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input : dataframe\n",
    "    dataframe de entrada con todos los textos.\n",
    "    lemmatizer : object\n",
    "    objeto del lematizador de NLTK.\n",
    "    word_vectors : object\n",
    "    objecto con los word2vec del vocabnbulario de Gensim.\n",
    "    vocabulary : list\n",
    "    lista con las palabras existentes en el vocabulario de Gensim.\n",
    "    col_sentences : str\n",
    "    columna del dataframe donde están las frases.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : list\n",
    "    Lista de listas en las que cada registro tiene la lista con los arrays de los embeddings de\n",
    "    las palabras de esa frase. \n",
    "    Es decir, X[0] tiene una lista donde cada elemento corresponde a los embeddings de una palabra. \n",
    "    Así, por ejemplo, X[0][2] será un vector de dimensión 100 donde aparece el vector de embeddings\n",
    "    de la tercera palabra de la primera frase.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    for text in df_input[col_sentences]:\n",
    "        # Tokenizo cada frase + Paso a minusculas todo\n",
    "        words = re.findall(r'\\w+', text.lower(),flags = re.UNICODE)\n",
    "        # Eliminación de las stopwords\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        # Elimino numeros\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "        #Lemmatization\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "        #Eliminar palabras que no esten en el vocabulario\n",
    "        words = [word for word in words if word in vocabulary]\n",
    "        #Word2Vec\n",
    "        words_embeddings = [word_vectors[x] for x in words]\n",
    "        #Guardo la frase final\n",
    "        X.append(words_embeddings) # lo guardo como un numpy array\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740b706",
   "metadata": {},
   "source": [
    "# II. Preprocess\n",
    "## Obtain X and Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2c7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_vector(df_raw, lemmatizer, word_vectors, vocabulary, col_sentences=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0631698",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5dc1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "df_raw['category'] = lb.fit_transform(df_raw['category'])\n",
    "y = df_raw['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7958a0d",
   "metadata": {},
   "source": [
    "## One-hot encode output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d1389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cc872",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e6fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448f14a",
   "metadata": {},
   "source": [
    "## Obtain tensor: [N_SENTENCES x SEQ_LENGTH x EMBEDDING_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "195ffec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = np.int(np.round(np.percentile([len(x) for x in X], 99, interpolation='midpoint')))\n",
    "\n",
    "data_train = pad_sequences(X_train, maxlen=SEQ_LENGTH, padding='post', truncating='post')\n",
    "data_test = pad_sequences(X_test, maxlen=SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0435633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN(X_train, K, n_lstm=8, loss='categorical_crossentropy', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Función para crear la RNN. Como parámetro de entrada sólo necesita la matriz de features\n",
    "    para especificar la dimensionalidad de entrada de la NN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_input : array\n",
    "    Matriz de features de entrada.\n",
    "    K: int\n",
    "    Clases de salida\n",
    "    n_lstm : int, optional\n",
    "    Number of lstm used. The default is 8.\n",
    "    loss : string, optional\n",
    "    Métrica de perdida. El default es 'categorical_crossentropy'.\n",
    "    optimizer : string, optional\n",
    "    Optimizador. El default es 'adam'.\n",
    "    \"\"\"\n",
    "    #Begin sequence\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #Add a LSTM layer with 8 internal units\n",
    "    model.add(LSTM(n_lstm, input_shape=X_train.shape[-2:]))\n",
    "    \n",
    "    #Output\n",
    "    model.add(Dense(K, activation='softmax'))\n",
    "    \n",
    "    #Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6404e5",
   "metadata": {},
   "source": [
    "# III. Train model\n",
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46eae27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = y_train.shape[1] # N classes\n",
    "batch_size = 200\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d6c3f",
   "metadata": {},
   "source": [
    "## Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf19ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 30,353\n",
      "Trainable params: 30,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_RNN(X_train = data_train, K = K, n_lstm=50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02dd05",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e1a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/model_nlp_reviews.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce26c5",
   "metadata": {},
   "source": [
    "# IV. Evaluate\n",
    "## Obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e1e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f857a80f",
   "metadata": {},
   "source": [
    "## Obtain original values (not one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d529eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [np.argmax(x) for x in y_test]\n",
    "y_pred = [np.argmax(x) for x in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1450954",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd7ca770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[104  15  35]\n",
      " [591  62  69]\n",
      " [268  24  44]]\n",
      "Precision:  0.3397\n",
      "Recall:  0.2974\n",
      "f1_score:  0.1729\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "print(\"Precision: \", np.round(precision_score(y_test, y_pred, average='macro'), 4))\n",
    "print(\"Recall: \", np.round(recall_score(y_test, y_pred, average='macro'), 4))\n",
    "print(\"f1_score: \", np.round(f1_score(y_test, y_pred, average='macro'), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18d81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
