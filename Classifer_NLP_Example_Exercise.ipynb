{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee4c6c9",
   "metadata": {},
   "source": [
    "# Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c6638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, log_loss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c023b",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a4afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(df_input, lemmatizer, word_vectors, vocabulary, col_sentences):\n",
    "    \"\"\"\n",
    "    Función para preprocesar las palabras de entrada y obtener una lista con las matrices de embeddings\n",
    "    de las palabras de cada registro.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input : dataframe\n",
    "    dataframe de entrada con todos los textos.\n",
    "    lemmatizer : object\n",
    "    objeto del lematizador de NLTK.\n",
    "    word_vectors : object\n",
    "    objecto con los word2vec del vocabulario de Gensim.\n",
    "    vocabulary : list\n",
    "    lista con las palabras existentes en el vocabulario de Gensim.\n",
    "    col_sentences : str\n",
    "    columna del dataframe donde están las frases.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    X : list\n",
    "    Lista de listas en las que cada registro tiene la lista con los arrays de los embeddings de\n",
    "    las palabras de esa frase. \n",
    "    Es decir, X[0] tiene una lista donde cada elemento corresponde a los embeddings de una palabra.\n",
    "    Así, por ejemplo, X[0][2] será un vector de dimensión 100 donde aparece el vector de embeddings de\n",
    "    la tercera palabra de la primera frase.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    \n",
    "    for text in df_input[col_sentences]:\n",
    "        # Tokenizo cada frase\n",
    "        words = re.findall(r'\\w+', text.lower(),flags = re.UNICODE) # Paso a minusculas todo\n",
    "        # Eliminación de las stop_words\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        # Elimino guiones y otros simbolos raros \n",
    "        words = [word for word in words if not word.isdigit()] # Elimino numeros\n",
    "        # Stemming\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "        # Eliminar palabras que no estén en el vocabulario\n",
    "        words = [word for word in words if word in vocabulary]\n",
    "        # Word2Vec\n",
    "        words_embeddings = [word_vectors[x] for x in words]\n",
    "        # Guardo la frase final\n",
    "        X.append(words_embeddings) # lo guardo como un numpy array\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN(x_train, K, n_lstm=8, loss='categorical_crossentropy', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Función para crear la RNN. Como parámetro de entrada sólo necesita la matriz de features para \n",
    "    especificar la dimensionalidad de entrada de la NN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_input : array\n",
    "    Matriz de features de entrada.\n",
    "    K: int\n",
    "    Clases de salida\n",
    "    n_lstm : int, optional\n",
    "    Number of lstm used. The default is 8.\n",
    "    loss : string, optional\n",
    "    loss metric. The default is 'categorical_crossentropy'.\n",
    "    optimizer : string, optional\n",
    "    optimizer. The default is 'adam'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : object\n",
    "    Trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Begin sequence\n",
    "    model = tf.keras.Sequential()\n",
    "    # Add a LSTM layer with 8 internal units.\n",
    "    model.add(LSTM(n_lstm, input_shape=x_train.shape[-2:]))\n",
    "    # Output\n",
    "    model.add(Dense(K, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea12fe",
   "metadata": {},
   "source": [
    "# I. Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff298c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "path='Data_Sources/NLP_Classifier_TrainData.csv'\n",
    "df_raw = pd.read_csv(path, encoding = 'latin-1', header=None, skipinitialspace=True, skiprows=1)\n",
    "df_transformed = df_raw.iloc[:, 3:]\n",
    "df_transformed.columns = ['text', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b449bd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three people died from the heat wave so far'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad474028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.57034\n",
       "1    0.42966\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed['category'].value_counts()/len(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d504ac",
   "metadata": {},
   "source": [
    "## Shuffle input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748e9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3a05c",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbac0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "vocabulary = [x for x in word_vectors.key_to_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e765e",
   "metadata": {},
   "source": [
    "## Set lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3b8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1bf96",
   "metadata": {},
   "source": [
    "## X/y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04862899",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df_transformed['text'])\n",
    "y = pd.to_numeric(df_transformed['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228bf9",
   "metadata": {},
   "source": [
    "# II. Preprocess\n",
    "## Obtain X variable and prepare y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c1f64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_vector(X, lemmatizer, word_vectors, vocabulary, col_sentences='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a55371",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd62a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abed23",
   "metadata": {},
   "source": [
    "## Obtain tensorr: [N_SENTENCES x SEQ_LENGTH x EMBEDDING_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245652fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = np.int(np.round(np.percentile([len(x) for x in X], 99, interpolation = 'midpoint')))\n",
    "data_train = pad_sequences(X_train, maxlen=SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "data_test = pad_sequences(X_test, maxlen=SEQ_LENGTH, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffeb1c",
   "metadata": {},
   "source": [
    "# III. Train model\n",
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a02cc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "batch_size = 50\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7add7",
   "metadata": {},
   "source": [
    "## Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e5264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 30,251\n",
      "Trainable params: 30,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_RNN(x_train = data_train, K = K, n_lstm = 50, loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a5155",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dbaf928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "115/115 [==============================] - 7s 17ms/step - loss: 0.5517\n",
      "Epoch 2/5\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4798\n",
      "Epoch 3/5\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4694\n",
      "Epoch 4/5\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4549\n",
      "Epoch 5/5\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb29c1105e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_train, y_train, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bee4395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7202    0\n",
       "1630    0\n",
       "4677    0\n",
       "3240    0\n",
       "2967    0\n",
       "       ..\n",
       "2963    0\n",
       "475     1\n",
       "7116    1\n",
       "7222    1\n",
       "1416    1\n",
       "Name: category, Length: 5709, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fc5e6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  1,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  1,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ..., -1,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21619116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3502    1\n",
       "5068    1\n",
       "3722    0\n",
       "6064    1\n",
       "3641    1\n",
       "       ..\n",
       "4186    0\n",
       "6068    1\n",
       "7124    1\n",
       "1746    0\n",
       "1203    0\n",
       "Name: category, Length: 7613, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06046fdb",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db11e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/model_nlp_disaster.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c074fd1",
   "metadata": {},
   "source": [
    "# IV. Evaluate\n",
    "## Obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2633db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91387aeb",
   "metadata": {},
   "source": [
    "## Round predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b57246",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b70cc721e474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#y_test = list(y_test.values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "#y_pred = y_pred.round()\n",
    "#y_pred = [x[0] for x in y_pred]\n",
    "#y_test = list(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fee2ca",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ec47c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[951 108]\n",
      " [318 527]]\n",
      "Precision:  0.7897\n",
      "Recall:  0.7608\n",
      "f1_score:  0.7646\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "print(\"Precision: \", np.round(precision_score(y_test, y_pred, average='macro'), 4))\n",
    "print(\"Recall: \", np.round(recall_score(y_test, y_pred, average='macro'), 4))\n",
    "print(\"f1_score: \", np.round(f1_score(y_test, y_pred, average='macro'), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b145373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
